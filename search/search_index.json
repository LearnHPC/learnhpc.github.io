{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to LearnHPC! \u00b6 What is this website for? \u00b6 The goal of this website is to ensure that High Performance Computing (HPC) is an accessible technology for the widest possible community of scientific researchers. We wish to act as a gateway to materials, resources and tools that will lower or remove as many barriers as possible in the learners journey when discovering High Performance Computing (HPC). We want to formulate a scalable HPC training approach that will facilitate the removal of any perception of elitism from HPC and expose the maximum amount of researchers possible to HPC resources, and the possibilities that they can enable. For those that continue beyond the introductory material you will find here, we will try to point you to the next level of HPC resources (and associated training). Intended audience \u00b6 The target audience for the final page is anyone who is motivated to learn more about HPC. Contributors to this page are likely to be people who are trying to cater to that audience. We are (mostly) linking to external content, so we try to keep the contextual information pretty brief, and the details you can find through the links. There are likely many biases in the way we present things here (and content we include), which reflect the activities and interests of the people who have contributed them. Hopefully these biases help more they hurt! Contents \u00b6 Scalable HPC Training - an overview of the LearnHPC approach Getting access to HPC resources. Via LearnHPC Accessing larger scale resources Providing a software environment Training material Introductory training HPC Carpentry Intermediate/advanced training PRACE HPC Centres of Excellence Contributors \u00b6 Alan O'Cais ( @ocaisa , J\u00fclich Supercomputing Centre, Germany )","title":"Home"},{"location":"#welcome-to-learnhpc","text":"","title":"Welcome to LearnHPC!"},{"location":"#what-is-this-website-for","text":"The goal of this website is to ensure that High Performance Computing (HPC) is an accessible technology for the widest possible community of scientific researchers. We wish to act as a gateway to materials, resources and tools that will lower or remove as many barriers as possible in the learners journey when discovering High Performance Computing (HPC). We want to formulate a scalable HPC training approach that will facilitate the removal of any perception of elitism from HPC and expose the maximum amount of researchers possible to HPC resources, and the possibilities that they can enable. For those that continue beyond the introductory material you will find here, we will try to point you to the next level of HPC resources (and associated training).","title":"What is this website for?"},{"location":"#intended-audience","text":"The target audience for the final page is anyone who is motivated to learn more about HPC. Contributors to this page are likely to be people who are trying to cater to that audience. We are (mostly) linking to external content, so we try to keep the contextual information pretty brief, and the details you can find through the links. There are likely many biases in the way we present things here (and content we include), which reflect the activities and interests of the people who have contributed them. Hopefully these biases help more they hurt!","title":"Intended audience"},{"location":"#contents","text":"Scalable HPC Training - an overview of the LearnHPC approach Getting access to HPC resources. Via LearnHPC Accessing larger scale resources Providing a software environment Training material Introductory training HPC Carpentry Intermediate/advanced training PRACE HPC Centres of Excellence","title":"Contents"},{"location":"#contributors","text":"Alan O'Cais ( @ocaisa , J\u00fclich Supercomputing Centre, Germany )","title":"Contributors"},{"location":"eessi/","text":"European Environment for Scientific Software Installations \u00b6 The European Environment for Scientific Software Installations ( EESSI , pronounced \"easy\") is a collaboration between a number of academic and industrial partners in the HPC community . Through the EESSI project, they want to set up a shared stack of scientific software installations to avoid not only duplicate work across HPC sites but also the execution of sub-optimal applications on HPC resources. They want to focus not only on the performance of the software, but also on automating the workflow for maintaining the software stack, thoroughly testing the installations, and collaborating efficiently. For end users, they want to provide a uniform user experience with respect to available scientific software, regardless of which system they use. The software stack is intended to work on laptops, personal workstations, HPC clusters and in the cloud, which means we will need to support different CPUs, networks, GPUs, and so on. We intend to make this work for any Linux distribution, and a wide variety of CPU architectures (Intel, AMD, ARM, POWER, RISC-V). Installing your own software \u00b6 As the EESSI platform develops, we will expand the documentation to include the case where you would like to install your software building upon the EESSI stack. Of course you are always free to install your software however you see fit, but you should be aware that (at least) two powerful tools exist to facilitate this, EasyBuild and Spack . EasyBuild \u00b6 EasyBuild is a software build and installation framework that allows you to manage (scientific) software on High Performance Computing (HPC) systems in an efficient way. It is motivated by the need for a tool that combines the following features: a flexible framework for building/installing (scientific) software fully automates software builds divert from the standard configure / make / make install with custom procedures allows for easily reproducing previous builds keep the software build recipes/specifications simple and human-readable supports co-existence of versions/builds via dedicated installation prefix and module files enables sharing with the HPC community (win-win situation) automagic dependency resolution retain logs for traceability of the build processes Since EasyBuild is what underpins the software applications of EESSI, you are likely to find the integration with the EESSI to be extensive. There is a one day introduction to EasyBuild at https://easybuilders.github.io/easybuild-tutorial/ Spack \u00b6 Spack is a package management tool designed to support multiple versions and configurations of software on a wide variety of platforms and environments. It was designed for large supercomputing centers, where many users and application teams share common installations of software on clusters with exotic architectures, using libraries that do not have a standard ABI. Spack is non-destructive: installing a new version does not break existing installations, so many configurations can coexist on the same system. You can find a one-day introduction to Spack tutorial at https://spack-tutorial.readthedocs.io/en/latest/","title":"Software Environment"},{"location":"eessi/#european-environment-for-scientific-software-installations","text":"The European Environment for Scientific Software Installations ( EESSI , pronounced \"easy\") is a collaboration between a number of academic and industrial partners in the HPC community . Through the EESSI project, they want to set up a shared stack of scientific software installations to avoid not only duplicate work across HPC sites but also the execution of sub-optimal applications on HPC resources. They want to focus not only on the performance of the software, but also on automating the workflow for maintaining the software stack, thoroughly testing the installations, and collaborating efficiently. For end users, they want to provide a uniform user experience with respect to available scientific software, regardless of which system they use. The software stack is intended to work on laptops, personal workstations, HPC clusters and in the cloud, which means we will need to support different CPUs, networks, GPUs, and so on. We intend to make this work for any Linux distribution, and a wide variety of CPU architectures (Intel, AMD, ARM, POWER, RISC-V).","title":"European Environment for Scientific Software Installations"},{"location":"eessi/#installing-your-own-software","text":"As the EESSI platform develops, we will expand the documentation to include the case where you would like to install your software building upon the EESSI stack. Of course you are always free to install your software however you see fit, but you should be aware that (at least) two powerful tools exist to facilitate this, EasyBuild and Spack .","title":"Installing your own software"},{"location":"eessi/#easybuild","text":"EasyBuild is a software build and installation framework that allows you to manage (scientific) software on High Performance Computing (HPC) systems in an efficient way. It is motivated by the need for a tool that combines the following features: a flexible framework for building/installing (scientific) software fully automates software builds divert from the standard configure / make / make install with custom procedures allows for easily reproducing previous builds keep the software build recipes/specifications simple and human-readable supports co-existence of versions/builds via dedicated installation prefix and module files enables sharing with the HPC community (win-win situation) automagic dependency resolution retain logs for traceability of the build processes Since EasyBuild is what underpins the software applications of EESSI, you are likely to find the integration with the EESSI to be extensive. There is a one day introduction to EasyBuild at https://easybuilders.github.io/easybuild-tutorial/","title":"EasyBuild"},{"location":"eessi/#spack","text":"Spack is a package management tool designed to support multiple versions and configurations of software on a wide variety of platforms and environments. It was designed for large supercomputing centers, where many users and application teams share common installations of software on clusters with exotic architectures, using libraries that do not have a standard ABI. Spack is non-destructive: installing a new version does not break existing installations, so many configurations can coexist on the same system. You can find a one-day introduction to Spack tutorial at https://spack-tutorial.readthedocs.io/en/latest/","title":"Spack"},{"location":"learnhpc/","text":"LearnHPC: dynamic creation of HPC infrastructure for educational purposes \u00b6 EU-wide requirements for HPC training are exploding as the adoption of HPC in the wider scientific community gathers pace. However, the number of topics that can be thoroughly addressed without providing access to actual HPC resources is very limited, even at the introductory level. In cases where such access is available, security concerns and the overhead of the process of provisioning accounts make the scalability of this approach questionable. EU-wide access to HPC resources on the scale required to meet the training needs of all countries is an objective that we attempt to address with this project. The proposed solution essentially provisions virtual HPC systems in a public cloud. This infrastructure will allow us to dynamically create temporary event-specific HPC clusters for training purposes, including a scientific software stack . The scientific software stack will be provided by the European Environment for Scientific Software Installations ( EESSI ) which uses a software distribution system developed at CERN, CernVM-FS, and makes a research-grade scalable software stack available for a wide set of HPC systems, as well as servers, desktops and laptops (including MacOS and Windows!). Magic Castle \u00b6 The concept is built upon the solution of Compute Canada , Magic Castle , which aims to recreate the Compute Canada user experience in public cloud. Magic Castle uses the open-source software Terraform and HashiCorp Language (HCL) to define the virtual machines, volumes, and networks that are required to replicate a virtual HPC infrastructure. Our adaption of Magic Castle aims to recreate the EESSI HPC user experience, for training purposes, primarily on the Fenix Research Infrastructure but can also leverage other cloud providers such as Azure, AWS, OVH and any provider using OpenStack for provisioning. After deployment, the user is provided with a complete HPC cluster software environment including a Slurm scheduler, a Globus Endpoint, JupyterHub, LDAP, DNS, and a wide selection of research software applications compiled by experts with EasyBuild. Accessing LearnHPC resources \u00b6 Through the FENIX Research Infrastructure and the generosity of AWS , LearnHPC can offer the use of moderately sized clusters configured specifically for your training events (and potentially with GPU and infiniband interconnect support). At present, we do not have a specific mechanism to request access to LearnHPC resources. To find out what is possible, please get in touch by email with Alan O'Cais ( alan@learnhpc.eu ) Accessing larger scale resources \u00b6 The resources we can provide are solely for training purposes. They are not intended to be long-lived or large scale. If you are in the EU and would like to get access to larger scale resources, particularly as you move from training to HPC usage, I would suggest you get in touch with the following projects who can help point you in the right direction: CASTIEL : CASTIEL, the Coordination and Support Action (CSA) closely associated with EuroCC, combines the National Competence Centres (NCC) formed in EuroCC into a pan-European network. CASTIEL should be able to link you to your National Competence Centre which can support you in your usage of HPC. PRACE : The mission of PRACE (Partnership for Advanced Computing in Europe) is to enable high-impact scientific discovery and engineering research and development across all disciplines to enhance European competitiveness for the benefit of society. PRACE seeks to realise this mission by offering world class computing and data management resources and services through a peer review process . PRACE also has connections to national HPC sites, in addition to providing various levels of access to a huge range of European HPC resources (including the largest HPC sites in Europe)","title":"Overview"},{"location":"learnhpc/#learnhpc-dynamic-creation-of-hpc-infrastructure-for-educational-purposes","text":"EU-wide requirements for HPC training are exploding as the adoption of HPC in the wider scientific community gathers pace. However, the number of topics that can be thoroughly addressed without providing access to actual HPC resources is very limited, even at the introductory level. In cases where such access is available, security concerns and the overhead of the process of provisioning accounts make the scalability of this approach questionable. EU-wide access to HPC resources on the scale required to meet the training needs of all countries is an objective that we attempt to address with this project. The proposed solution essentially provisions virtual HPC systems in a public cloud. This infrastructure will allow us to dynamically create temporary event-specific HPC clusters for training purposes, including a scientific software stack . The scientific software stack will be provided by the European Environment for Scientific Software Installations ( EESSI ) which uses a software distribution system developed at CERN, CernVM-FS, and makes a research-grade scalable software stack available for a wide set of HPC systems, as well as servers, desktops and laptops (including MacOS and Windows!).","title":"LearnHPC: dynamic creation of HPC infrastructure for educational purposes"},{"location":"learnhpc/#magic-castle","text":"The concept is built upon the solution of Compute Canada , Magic Castle , which aims to recreate the Compute Canada user experience in public cloud. Magic Castle uses the open-source software Terraform and HashiCorp Language (HCL) to define the virtual machines, volumes, and networks that are required to replicate a virtual HPC infrastructure. Our adaption of Magic Castle aims to recreate the EESSI HPC user experience, for training purposes, primarily on the Fenix Research Infrastructure but can also leverage other cloud providers such as Azure, AWS, OVH and any provider using OpenStack for provisioning. After deployment, the user is provided with a complete HPC cluster software environment including a Slurm scheduler, a Globus Endpoint, JupyterHub, LDAP, DNS, and a wide selection of research software applications compiled by experts with EasyBuild.","title":"Magic Castle"},{"location":"learnhpc/#accessing-learnhpc-resources","text":"Through the FENIX Research Infrastructure and the generosity of AWS , LearnHPC can offer the use of moderately sized clusters configured specifically for your training events (and potentially with GPU and infiniband interconnect support). At present, we do not have a specific mechanism to request access to LearnHPC resources. To find out what is possible, please get in touch by email with Alan O'Cais ( alan@learnhpc.eu )","title":"Accessing LearnHPC resources"},{"location":"learnhpc/#accessing-larger-scale-resources","text":"The resources we can provide are solely for training purposes. They are not intended to be long-lived or large scale. If you are in the EU and would like to get access to larger scale resources, particularly as you move from training to HPC usage, I would suggest you get in touch with the following projects who can help point you in the right direction: CASTIEL : CASTIEL, the Coordination and Support Action (CSA) closely associated with EuroCC, combines the National Competence Centres (NCC) formed in EuroCC into a pan-European network. CASTIEL should be able to link you to your National Competence Centre which can support you in your usage of HPC. PRACE : The mission of PRACE (Partnership for Advanced Computing in Europe) is to enable high-impact scientific discovery and engineering research and development across all disciplines to enhance European competitiveness for the benefit of society. PRACE seeks to realise this mission by offering world class computing and data management resources and services through a peer review process . PRACE also has connections to national HPC sites, in addition to providing various levels of access to a huge range of European HPC resources (including the largest HPC sites in Europe)","title":"Accessing larger scale resources"}]}